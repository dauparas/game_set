{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_game_set.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dauparas/game_set/blob/master/cnn_game_set.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1yxSv9Jt_b3",
        "colab_type": "text"
      },
      "source": [
        "CNN notebook is inspired by\n",
        "https://github.com/rasbt/deeplearning-models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O069ap7Hj2BS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa7f8016-b183-4974-981d-f18b4d182acc"
      },
      "source": [
        "import tensorflow as tf\n",
        "from functools import reduce\n",
        "import numpy as np\n",
        "tf.test.is_gpu_available()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_CxgfvYieV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "fbab4db4-29b6-427d-a7be-6245d6a05203"
      },
      "source": [
        "#Get the dataset\n",
        "!git clone https://github.com/dauparas/game_set.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'game_set'...\n",
            "remote: Enumerating objects: 103, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 103 (delta 9), reused 87 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (103/103), 28.19 MiB | 49.68 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-mx2DdrjSC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create labels for the dataset; 4 categories with 3 possibilities\n",
        "import numpy as np\n",
        "#Read Image\n",
        "filenames = [] #create a list of filenames\n",
        "labels = np.zeros([1, 4]) #create an array of labels\n",
        "for i in range(3):\n",
        "  for j in range(3):\n",
        "    for k in range(3):\n",
        "      for l in range(3):\n",
        "        filenames.append(str(i+1)+str(j+1)+str(k+1)+str(l+1)+'.jpg')\n",
        "        _lab = np.array([i+1, j+1, k+1, l+1]).reshape(1, 4)\n",
        "        labels = np.concatenate((labels, _lab), axis=0)\n",
        "        \n",
        "labels = labels[1:,:].astype(np.int32)-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlforKVgj0bF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_function(filename, label):\n",
        "    image_string = tf.read_file(\"./game_set/original_81/\" + filename)\n",
        "\n",
        "    # Don't use tf.image.decode_image, or the output shape will be undefined\n",
        "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
        "\n",
        "    # This will convert to float values in [0, 1]\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    \n",
        "    image = tf.image.resize_images(image, [100, 160])\n",
        "    return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mmAIYRGkkB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_preprocess(image, label):\n",
        "    \"Data augmentation\"\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n",
        "    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "\n",
        "    # Make sure the image is still in [0, 1]\n",
        "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
        "\n",
        "    return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgfaHq6VbIv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################\n",
        "### SETTINGS\n",
        "##########################\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 5\n",
        "batch_size = 32\n",
        "\n",
        "# Architecture\n",
        "input_size = 160*100\n",
        "image_width = 160\n",
        "image_height = 100\n",
        "\n",
        "# Other\n",
        "random_seed = 0\n",
        "\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "    \n",
        "    tf.set_random_seed(random_seed)\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "    dataset = dataset.shuffle(len(filenames))\n",
        "    dataset = dataset.map(parse_function, num_parallel_calls=4)\n",
        "    dataset = dataset.map(train_preprocess, num_parallel_calls=4)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(1)\n",
        "\n",
        "    iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\n",
        "    input_layer, y = iterator.get_next()\n",
        "    train_init = iterator.make_initializer(dataset)\t# initializer for train_data\n",
        "\n",
        "    # Input image\n",
        "    #input_layer = tf.placeholder(tf.float32, [None, image_width, image_height, 3], name='inputs')\n",
        "    \n",
        "    # Input correct labels\n",
        "    #y = tf.placeholder(tf.int32, [None, 4], name='y')\n",
        "\n",
        "    ###########\n",
        "    # Encoder\n",
        "    ###########\n",
        "    \n",
        "    # 160x100x1 => 160x100x24\n",
        "    conv1 = tf.layers.conv2d(input_layer, filters=24, kernel_size=(5, 5),\n",
        "                             strides=(1, 1), padding='same', \n",
        "                             activation=tf.nn.relu)\n",
        "    # 160x100x24 => 80x50x24\n",
        "    maxpool1 = tf.layers.max_pooling2d(conv1, pool_size=(2, 2), \n",
        "                                       strides=(2, 2), padding='same')\n",
        "    # 80x50x24 => 80x50x12\n",
        "    conv2 = tf.layers.conv2d(maxpool1, filters=12, kernel_size=(5, 5), \n",
        "                             strides=(1, 1), padding='same', \n",
        "                             activation=tf.nn.relu)\n",
        "    # 80x50x24 => 40x25x12\n",
        "    maxpool2 = tf.layers.max_pooling2d(conv2, pool_size=(2, 2), \n",
        "                                     strides=(2, 2), padding='same', \n",
        "                                     name='encoding')\n",
        "    \n",
        "    # 40x25x12 => 40x25x12\n",
        "    conv3 = tf.layers.conv2d(maxpool2, filters=12, kernel_size=(5, 5), \n",
        "                             strides=(1, 1), padding='same', \n",
        "                             activation=tf.nn.relu)\n",
        "    # 40x25x12 => 20x13x12\n",
        "    maxpool3 = tf.layers.max_pooling2d(conv3, pool_size=(2, 2), \n",
        "                                     strides=(2, 2), padding='same', \n",
        "                                     name='encoding')\n",
        "    #20x13x12 -> 20x13x12\n",
        "    conv4 = tf.layers.conv2d(maxpool3, filters=12, kernel_size=(5, 5), \n",
        "                             strides=(1, 1), padding='same', \n",
        "                             activation=tf.nn.relu)\n",
        "    #20x13x12 => 10x7x12\n",
        "    encode = tf.layers.max_pooling2d(conv4, pool_size=(2, 2), \n",
        "                                     strides=(2, 2), padding='same', \n",
        "                                     name='encoding')\n",
        "    \n",
        "    \n",
        "    encode = tf.reshape(encode, (tf.shape(input_layer)[0], 840))\n",
        "    dense = tf.layers.dense(encode, 32, activation=tf.nn.relu)\n",
        "    \n",
        "    g1 = tf.layers.dense(encode, 3, activation=None) #number\n",
        "    g2 = tf.layers.dense(encode, 3, activation=None) #shading\n",
        "    g3 = tf.layers.dense(encode, 3, activation=None) #color\n",
        "    g4 = tf.layers.dense(encode, 3, activation=None) #shape\n",
        "    \n",
        "    \n",
        "    loss1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y[:,0], logits=g1, name='l1'))\n",
        "    loss2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y[:,1], logits=g2, name='l2'))\n",
        "    loss3 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y[:,2], logits=g3, name='l3'))\n",
        "    loss4 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y[:,3], logits=g4, name='l4'))\n",
        "\n",
        "    loss = loss1 + loss2 + loss3 + loss4\n",
        "    cost = tf.reduce_mean(loss, name='cost')\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "    train = optimizer.minimize(cost, name='train')    \n",
        "    \n",
        "    # Saver to save session for reuse\n",
        "    saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfHAYf9xqeHM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "06a41e75-6fb8-45a4-bcd5-5b46f24afae4"
      },
      "source": [
        "#Create a TF session:\n",
        "with tf.Session(graph=g) as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for i in range(500):\n",
        "    sess.run(train_init)\n",
        "    total_loss = 0\n",
        "    n_batches = 0\n",
        "    try:\n",
        "      while True:\n",
        "        _, batch_loss = sess.run([train, loss])\n",
        "        total_loss += batch_loss\n",
        "        n_batches += 1\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      pass\n",
        "    if (i+1)% 10 == 0 or i == 0:\n",
        "      print('Epoch: {0}, Loss: {1}'.format((i+1), total_loss))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 13.267520427703857\n",
            "Epoch: 10, Loss: 7.965855598449707\n",
            "Epoch: 20, Loss: 4.582868576049805\n",
            "Epoch: 30, Loss: 2.8290826082229614\n",
            "Epoch: 40, Loss: 2.22158682346344\n",
            "Epoch: 50, Loss: 1.2740881443023682\n",
            "Epoch: 60, Loss: 2.576905846595764\n",
            "Epoch: 70, Loss: 1.3859631717205048\n",
            "Epoch: 80, Loss: 0.5280037373304367\n",
            "Epoch: 90, Loss: 0.4536770358681679\n",
            "Epoch: 100, Loss: 0.22381533682346344\n",
            "Epoch: 110, Loss: 0.10528392903506756\n",
            "Epoch: 120, Loss: 0.13971873559057713\n",
            "Epoch: 130, Loss: 0.14994891453534365\n",
            "Epoch: 140, Loss: 0.07379152625799179\n",
            "Epoch: 150, Loss: 0.2363655548542738\n",
            "Epoch: 160, Loss: 3.797917425632477\n",
            "Epoch: 170, Loss: 0.43973132222890854\n",
            "Epoch: 180, Loss: 0.38372571766376495\n",
            "Epoch: 190, Loss: 0.14156026393175125\n",
            "Epoch: 200, Loss: 0.3214944712817669\n",
            "Epoch: 210, Loss: 0.1960030561313033\n",
            "Epoch: 220, Loss: 0.169401363003999\n",
            "Epoch: 230, Loss: 0.04993235459551215\n",
            "Epoch: 240, Loss: 0.014396531973034143\n",
            "Epoch: 250, Loss: 0.009653221000917256\n",
            "Epoch: 260, Loss: 0.010752371163107455\n",
            "Epoch: 270, Loss: 0.009069980005733669\n",
            "Epoch: 280, Loss: 0.011070718057453632\n",
            "Epoch: 290, Loss: 0.006417135940864682\n",
            "Epoch: 300, Loss: 0.006580130546353757\n",
            "Epoch: 310, Loss: 0.004113881557714194\n",
            "Epoch: 320, Loss: 0.003756613004952669\n",
            "Epoch: 330, Loss: 0.005039319104980677\n",
            "Epoch: 340, Loss: 0.02154345641611144\n",
            "Epoch: 350, Loss: 0.036751624778844416\n",
            "Epoch: 360, Loss: 0.010783379490021616\n",
            "Epoch: 370, Loss: 0.00727918790653348\n",
            "Epoch: 380, Loss: 0.004376451775897294\n",
            "Epoch: 390, Loss: 0.0023924204288050532\n",
            "Epoch: 400, Loss: 0.0034768189070746303\n",
            "Epoch: 410, Loss: 0.002671789232408628\n",
            "Epoch: 420, Loss: 0.0027730552246794105\n",
            "Epoch: 430, Loss: 0.003296314855106175\n",
            "Epoch: 440, Loss: 0.0015362365229520947\n",
            "Epoch: 450, Loss: 0.002040731080342084\n",
            "Epoch: 460, Loss: 0.001957007116288878\n",
            "Epoch: 470, Loss: 8.853482246398926\n",
            "Epoch: 480, Loss: 0.5844961144030094\n",
            "Epoch: 490, Loss: 0.05183915654197335\n",
            "Epoch: 500, Loss: 0.032483189133927226\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}